{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 6: Two-Model Approach (CatBoost CLF + LightGBM REG)\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "This approach uses two separate models for classification and regression, respectively:\n",
    "1. **CatBoost Classifier**: Predicts death probability (with sample weights)\n",
    "2. **LightGBM Regressor**: Predicts survival time (trained on events only)\n",
    "\n",
    "This is inspired by the 4th place solution in the Kaggle competition CIBMTR, which the author generously made public online.\n",
    "\n",
    "## Sample Weighting\n",
    "- **Events (deaths)**: weight = 1.0\n",
    "- **Censored**: weight = F(t) / F_max (KM cumulative density at censoring time)\n",
    "\n",
    "\n",
    "## Merge Formula\n",
    "```\n",
    "risk = pred_event × (1 + odds(avg_pred_event) × (1 - pred_time_norm))\n",
    "```\n",
    "\n",
    "## Configuration\n",
    "- **Features**: 128 fixed (NaN imputed, scaled)\n",
    "- **Evaluation**: `concordance_index_ipcw` from sksurv\n",
    "- **CV Score**: 0.6909 weighted C-index\n",
    "- **Weight in 4-model ensemble**: 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "from sksurv.util import Surv\n",
    "from lifelines import KaplanMeierFitter\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "TRAIN_PATH = '/your_path/SurvivalPrediction/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 128\n",
      "Samples: 3120\n",
      "Events: 1600 (51.3%)\n"
     ]
    }
   ],
   "source": [
    "# Load 128-feature FIXED scaled dataset\n",
    "X_train = pd.read_csv(f'{TRAIN_PATH}/X_train_128features_clean_fixed_scaled.csv')\n",
    "X_train_unscaled = pd.read_csv(f'{TRAIN_PATH}/X_train_128features_clean_fixed.csv')\n",
    "target = pd.read_csv(f'{TRAIN_PATH}/target_train_clean_aligned.csv')\n",
    "\n",
    "y_time = target['OS_YEARS'].values\n",
    "y_event = target['OS_STATUS'].values.astype(bool)\n",
    "n_samples = len(X_train)\n",
    "\n",
    "# Create structured array for sksurv\n",
    "y_surv = Surv.from_arrays(event=y_event, time=y_time)\n",
    "\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Samples: {n_samples}\")\n",
    "print(f\"Events: {y_event.sum()} ({y_event.mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk groups\n",
    "def define_risk_groups(X_unscaled):\n",
    "    risk_factors = pd.DataFrame(index=X_unscaled.index)\n",
    "    risk_factors['high_blast'] = (X_unscaled['BM_BLAST'] > 10).astype(int)\n",
    "    risk_factors['has_TP53'] = (X_unscaled['has_TP53'] > 0).astype(int)\n",
    "    risk_factors['low_hb'] = (X_unscaled['HB'] < 10).astype(int)\n",
    "    risk_factors['low_plt'] = (X_unscaled['PLT'] < 50).astype(int)\n",
    "    risk_factors['high_cyto'] = (X_unscaled['cyto_risk_score'] >= 3).astype(int)\n",
    "    n_risk_factors = risk_factors.sum(axis=1)\n",
    "    return {\n",
    "        'test_like': n_risk_factors >= 1,\n",
    "        'high_risk': n_risk_factors >= 2,\n",
    "    }\n",
    "\n",
    "risk_groups = define_risk_groups(X_train_unscaled)\n",
    "\n",
    "# Stratification variable\n",
    "has_tp53 = (X_train_unscaled['has_TP53'] > 0).astype(int).values\n",
    "strat_var = pd.Series([f\"{int(e)}_{int(t)}\" for e, t in zip(y_event, has_tp53)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_cindex_ipcw(risk, y_surv_all, risk_groups, tau=7.0):\n",
    "    \"\"\"Compute weighted C-index using concordance_index_ipcw (competition metric).\"\"\"\n",
    "    c_overall = concordance_index_ipcw(y_surv_all, y_surv_all, risk, tau=tau)[0]\n",
    "    \n",
    "    mask_test = risk_groups['test_like'].values\n",
    "    y_surv_test = Surv.from_arrays(event=y_surv_all['event'][mask_test], time=y_surv_all['time'][mask_test])\n",
    "    c_test = concordance_index_ipcw(y_surv_all, y_surv_test, risk[mask_test], tau=tau)[0]\n",
    "\n",
    "    mask_high = risk_groups['high_risk'].values if hasattr(risk_groups['high_risk'], 'values') else risk_groups['high_risk']\n",
    "    y_surv_high = Surv.from_arrays(event=y_surv_all['event'][mask_high], time=y_surv_all['time'][mask_high])\n",
    "    c_high = concordance_index_ipcw(y_surv_all, y_surv_high, risk[mask_high], tau=tau)[0]\n",
    "\n",
    "    weighted = 0.3 * c_overall + 0.4 * c_test + 0.3 * c_high\n",
    "    return {'overall': c_overall, 'test_like': c_test, 'high_risk': c_high, 'weighted': weighted}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Sample Weighting\n",
    "\n",
    "Use Kaplan-Meier-based sample weights for the classifier:\n",
    "- **Events (deaths)**: weight = 1.0 (full weight)\n",
    "- **Censored observations**: weight = F(t) / F_max (cumulative density at censoring time / max cumulative density)\n",
    "\n",
    "This gives higher weight to censored patients who were observed longer (more informative).\n",
    "\n",
    "The LightGBM regressor is trained on **events only**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Sample Weighting:\n",
      "  Events: weight = 1.0\n",
      "  Censored: weight = F(t) / F_max (KM cumulative density)\n",
      "\n",
      "Merge Formula:\n",
      "  risk = pred_event * (1 + odds(avg_pred_event) * (1 - pred_time_norm))\n"
     ]
    }
   ],
   "source": [
    "# Sample weight computation\n",
    "def compute_sample_weights(times, events):\n",
    "    \"\"\"\n",
    "    Events (deaths): weight = 1.0\n",
    "    Censored: weight = F(t) / F_max (KM cumulative density)\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit Kaplan_Meier curve\n",
    "    kmf_event = KaplanMeierFitter()\n",
    "    kmf_event.fit(times, event_observed=events)\n",
    "    \n",
    "    # Get maximum cdf\n",
    "    t_max = times.max() # maximum observed time\n",
    "    F_max = kmf_event.cumulative_density_at_times([t_max]).values[0] #maximum cdf value\n",
    "    F_max = max(F_max, 0.01) # clipping to avoid division by zero\n",
    "\n",
    "    # Assign weights to samples\n",
    "    weights = np.zeros(len(times))\n",
    "    for i in range(len(times)):\n",
    "        if events[i] == 1:\n",
    "            weights[i] = 1.0\n",
    "        else:\n",
    "            F_t = kmf_event.cumulative_density_at_times([times[i]]).values[0]\n",
    "            weights[i] = F_t / F_max\n",
    "            \n",
    "    # normalize so average weight = 1.0\n",
    "    weights = weights / weights.mean()\n",
    "    return weights\n",
    "\n",
    "# Merge function: combine classifier and regressor predictions\n",
    "def merge_predictions(clf_pred, reg_pred, time_min, time_max):\n",
    "    # normalized predicted times then clip to [0,1]\n",
    "    pred_time_norm = (reg_pred - time_min) / (time_max - time_min + 1e-8)\n",
    "    pred_time_norm = np.clip(pred_time_norm, 0, 1) \n",
    "\n",
    "    # compute odds of predicted population deaths\n",
    "    avg_pred_event = np.mean(clf_pred)\n",
    "    odds = avg_pred_event / (1 - avg_pred_event + 1e-8)\n",
    "    odds = np.clip(odds, 0.1, 10)\n",
    "\n",
    "    # compute score\n",
    "    risk = clf_pred * (1 + odds * (1 - pred_time_norm))\n",
    "    return risk\n",
    "\n",
    "print(\"Classification Sample Weighting:\")\n",
    "print(\"  Events: weight = 1.0\")\n",
    "print(\"  Censored: weight = F(t) / F_max (KM cumulative density)\")\n",
    "print(\"\\nMerge Formula:\")\n",
    "print(\"  risk = pred_event * (1 + odds(avg_pred_event) * (1 - pred_time_norm))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Global OOF Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_oof_evaluate(cat_params, lgb_params, n_splits=5, base_seed=42):\n",
    "    \"\"\"Global OOF CV: CatBoost CLF (sample weights) + LightGBM REG (events only).\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=base_seed)\n",
    "    oof_preds = np.zeros(n_samples)\n",
    "    X_arr = X_train.values\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_arr, strat_var)):\n",
    "        seed = base_seed + fold_idx\n",
    "        X_tr, X_val = X_arr[train_idx], X_arr[val_idx]\n",
    "        y_time_tr = y_time[train_idx]\n",
    "        y_event_tr = y_event[train_idx].astype(int)\n",
    "\n",
    "        # Sample weights for classifier; estimate from only training folds to avoid data leakage\n",
    "        clf_weights = compute_sample_weights(y_time_tr, y_event_tr)\n",
    "\n",
    "        # CatBoost classifier with sample weights\n",
    "        cat_params_fold = {**cat_params, 'random_seed': seed}\n",
    "        clf = CatBoostClassifier(**cat_params_fold)\n",
    "        clf.fit(X_tr, y_event_tr, sample_weight=clf_weights)\n",
    "        clf_pred = clf.predict_proba(X_val)[:, 1] # pick second column for predicted P(death)\n",
    "\n",
    "        # LightGBM regressor (events only)\n",
    "        event_mask = y_event_tr == 1\n",
    "        X_tr_events = X_tr[event_mask]\n",
    "        y_time_events = y_time_tr[event_mask]\n",
    "\n",
    "        lgb_params_fold = {**lgb_params, 'random_state': seed, 'verbosity': -1}\n",
    "        reg = lgb.LGBMRegressor(**lgb_params_fold)\n",
    "        reg.fit(X_tr_events, y_time_events)\n",
    "        reg_pred = reg.predict(X_val)\n",
    "\n",
    "        # Merge predictions\n",
    "        time_min, time_max = y_time_tr.min(), y_time_tr.max()\n",
    "        risk = merge_predictions(clf_pred, reg_pred, time_min, time_max)\n",
    "        oof_preds[val_idx] = risk\n",
    "\n",
    "    # Global Z-score normalization\n",
    "    oof_normalized = (oof_preds - oof_preds.mean()) / (oof_preds.std() + 1e-8)\n",
    "    return weighted_cindex_ipcw(oof_normalized, y_surv, risk_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Optuna hyperparameter tuning...\n",
      "(Set n_trials=200 for full tuning)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f570efe3dec441780e1a7d91ca06a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best trial:\n",
      "  Weighted C-index: 0.6912\n",
      "  Params: {'cat_depth': 5, 'cat_iterations': 262, 'cat_learning_rate': 0.019227204273246305, 'cat_l2_leaf_reg': 0.12876998314647772, 'lgb_max_depth': 9, 'lgb_n_estimators': 149, 'lgb_learning_rate': 0.08025255147825751, 'lgb_num_leaves': 36, 'lgb_min_child_samples': 10, 'lgb_subsample': 0.9638486108112962, 'lgb_colsample_bytree': 0.8126162431458441, 'lgb_reg_alpha': 4.6796458896222854e-06, 'lgb_reg_lambda': 4.190436671160808e-07}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Optuna objective for Two-Model hyperparameter tuning.\"\"\"\n",
    "    cat_params = {\n",
    "        'depth': trial.suggest_int('cat_depth', 3, 8),\n",
    "        'iterations': trial.suggest_int('cat_iterations', 100, 400),\n",
    "        'learning_rate': trial.suggest_float('cat_learning_rate', 0.01, 0.1, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('cat_l2_leaf_reg', 0.1, 10, log=True),\n",
    "        'verbose': False,\n",
    "        'allow_writing_files': False,\n",
    "    }\n",
    "\n",
    "    lgb_params = {\n",
    "        'max_depth': trial.suggest_int('lgb_max_depth', 3, 10),\n",
    "        'n_estimators': trial.suggest_int('lgb_n_estimators', 100, 400),\n",
    "        'learning_rate': trial.suggest_float('lgb_learning_rate', 0.01, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('lgb_num_leaves', 15, 63),\n",
    "        'min_child_samples': trial.suggest_int('lgb_min_child_samples', 5, 50),\n",
    "        'subsample': trial.suggest_float('lgb_subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('lgb_colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('lgb_reg_alpha', 1e-8, 10, log=True),\n",
    "        'reg_lambda': trial.suggest_float('lgb_reg_lambda', 1e-8, 10, log=True),\n",
    "    }\n",
    "\n",
    "    results = global_oof_evaluate(cat_params, lgb_params)\n",
    "\n",
    "    trial.set_user_attr('overall', results['overall'])\n",
    "    trial.set_user_attr('test_like', results['test_like'])\n",
    "    trial.set_user_attr('high_risk', results['high_risk'])\n",
    "\n",
    "    return results['weighted']\n",
    "\n",
    "# Run Optuna study (reduced trials for notebook)\n",
    "print(\"Running Optuna hyperparameter tuning...\")\n",
    "print(\"(Set n_trials=200 for full tuning)\\n\")\n",
    "\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=200, show_progress_bar=True)  # Use 200 for full tuning\n",
    "\n",
    "print(f\"\\nBest trial:\")\n",
    "print(f\"  Weighted C-index: {study.best_value:.4f}\")\n",
    "print(f\"  Params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-Model Results (concordance_index_ipcw):\n",
      "  Overall C-index: 0.7184\n",
      "  Test-like C-index: 0.6937\n",
      "  High-risk C-index: 0.6608\n",
      "  Weighted C-index: 0.6912\n"
     ]
    }
   ],
   "source": [
    "BEST_CAT_PARAMS = {\n",
    "    'depth': 5, \n",
    "    'iterations': 262, \n",
    "    'learning_rate': 0.019227204273246305, \n",
    "    'l2_leaf_reg': 0.12876998314647772,\n",
    "    'verbose': False,\n",
    "    'allow_writing_files': False,\n",
    "    }\n",
    "\n",
    "BEST_LGB_PARAMS = {\n",
    "    'max_depth': 9, \n",
    "    'n_estimators': 149, \n",
    "    'learning_rate': 0.08025255147825751, \n",
    "    'num_leaves': 36, \n",
    "    'min_child_samples': 10, \n",
    "    'subsample': 0.9638486108112962, \n",
    "    'colsample_bytree': 0.8126162431458441, \n",
    "    'reg_alpha': 4.6796458896222854e-06, \n",
    "    'reg_lambda': 4.190436671160808e-07}\n",
    "\n",
    "result = global_oof_evaluate(BEST_CAT_PARAMS, BEST_LGB_PARAMS)\n",
    "print(\"Two-Model Results (concordance_index_ipcw):\")\n",
    "print(f\"  Overall C-index: {result['overall']:.4f}\")\n",
    "print(f\"  Test-like C-index: {result['test_like']:.4f}\")\n",
    "print(f\"  High-risk C-index: {result['high_risk']:.4f}\")\n",
    "print(f\"  Weighted C-index: {result['weighted']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Two-Model Results\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Overall C-index | 0.7184 |\n",
    "| Test-like C-index | 0.6937 |\n",
    "| High-risk C-index | 0.6608 |\n",
    "| **Weighted C-index** | **0.6912** |\n",
    "\n",
    "### Design\n",
    "- **CatBoost Classifier only**: Predicts death probability with KM-based sample weights\n",
    "- **LightGBM Regressor only**: Predicts survival time, trained on events only (censored times are lower bounds)\n",
    "- **Merge**: `risk = pred_event × (1 + odds(avg_pred_event) × (1 - pred_time_norm))`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
