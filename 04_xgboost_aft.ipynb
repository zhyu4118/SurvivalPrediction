{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: XGBoost Accelerated Failure Time (AFT)\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "XGBoost AFT is a gradient boosting model for survival analysis:\n",
    "- **Objective**: `survival:aft` (Accelerated Failure Time)\n",
    "- **Loss**: Negative log-likelihood of AFT distribution\n",
    "- **Handles censoring**: Via interval regression (lower/upper bounds)\n",
    "- **Handles NaN**: Native missing value handling\n",
    "\n",
    "## Configuration\n",
    "- **Features**: 83 unfixed (XGBoost handles NaN natively)\n",
    "- **Distribution**: Normal/Logistic/Extreme\n",
    "- **Tuning**: Optuna (100 trials)\n",
    "- **Evaluation**: `concordance_index_ipcw` from sksurv\n",
    "- **CV Score**: 0.6964 weighted C-index (BEST single model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "from sksurv.util import Surv\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# Paths\n",
    "TRAIN_PATH = '/your_path/SurvivalPrediction/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 83\n",
      "Samples: 3120\n",
      "Events: 1600 (51.3%)\n",
      "NaN values: 240 (XGBoost handles these natively)\n"
     ]
    }
   ],
   "source": [
    "# Load 83-feature UNFIXED dataset (XGBoost handles NaN)\n",
    "X_train_with_id = pd.read_csv(f'{TRAIN_PATH}/X_train_83features_with_id.csv')\n",
    "target = pd.read_csv(f'{TRAIN_PATH}/target_train_clean_aligned.csv')\n",
    "\n",
    "# Align to target\n",
    "X_train_with_id = X_train_with_id.set_index('ID').loc[target['ID']].reset_index()\n",
    "X_train = X_train_with_id.drop(columns=['ID'])\n",
    "\n",
    "y_time = target['OS_YEARS'].values\n",
    "y_event = target['OS_STATUS'].values.astype(bool)\n",
    "n_samples = len(X_train)\n",
    "\n",
    "# Create structured array for IPCW C-index\n",
    "y_surv = Surv.from_arrays(event=y_event, time=y_time)\n",
    "\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Samples: {n_samples}\")\n",
    "print(f\"Events: {y_event.sum()} ({y_event.mean()*100:.1f}%)\")\n",
    "print(f\"NaN values: {X_train.isna().sum().sum()} (XGBoost handles these natively)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-like subgroup: 2192 samples\n",
      "High-risk subgroup: 772 samples\n"
     ]
    }
   ],
   "source": [
    "# Risk groups for weighted C-index\n",
    "def define_risk_groups(X):\n",
    "    risk_factors = pd.DataFrame(index=X.index)\n",
    "    risk_factors['high_blast'] = (X['BM_BLAST'] > 10).astype(int)\n",
    "    risk_factors['has_TP53'] = (X['has_TP53'] > 0).astype(int)\n",
    "    risk_factors['low_hb'] = (X['HB'] < 10).astype(int)\n",
    "    risk_factors['low_plt'] = (X['PLT'] < 50).astype(int)\n",
    "    risk_factors['high_cyto'] = (X['cyto_risk_score'] >= 3).astype(int)\n",
    "    n_risk_factors = risk_factors.sum(axis=1)\n",
    "    return {\n",
    "        'test_like': n_risk_factors >= 1,\n",
    "        'high_risk': n_risk_factors >= 2,\n",
    "    }\n",
    "\n",
    "risk_groups = define_risk_groups(X_train)\n",
    "\n",
    "# Stratification variable for CV\n",
    "has_tp53 = (X_train['has_TP53'] > 0).astype(int).values\n",
    "strat_var = pd.Series([f\"{int(e)}_{int(t)}\" for e, t in zip(y_event, has_tp53)])\n",
    "\n",
    "print(f\"Test-like subgroup: {risk_groups['test_like'].sum()} samples\")\n",
    "print(f\"High-risk subgroup: {risk_groups['high_risk'].sum()} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AFT Model Explanation\n",
    "\n",
    "### Accelerated Failure Time Model\n",
    "\n",
    "AFT models assume: log(T) = f(X) + σε\n",
    "\n",
    "Where:\n",
    "- T = survival time\n",
    "- f(X) = XGBoost prediction (tree ensemble)\n",
    "- σ = scale parameter\n",
    "- ε = error term from specified distribution\n",
    "\n",
    "### Distribution Options\n",
    "- **normal**: ε ~ Normal(0,1) → log-normal survival times\n",
    "- **logistic**: ε ~ Logistic(0,1) → log-logistic survival times  \n",
    "- **extreme**: ε ~ Extreme Value → Weibull survival times\n",
    "\n",
    "### Handling Censoring\n",
    "XGBoost AFT uses interval censoring:\n",
    "- **Events**: lower_bound = upper_bound = observed_time\n",
    "- **Censored**: lower_bound = observed_time, upper_bound = +∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFT Label Bounds:\n",
      "  Events: lower=upper (exact time)\n",
      "  Censored: lower=time, upper=inf (right censored)\n",
      "\n",
      "  Events: 1600\n",
      "  Censored: 1520\n"
     ]
    }
   ],
   "source": [
    "# Prepare AFT labels\n",
    "y_lower = y_time.copy()\n",
    "y_upper = np.where(y_event, y_time, np.inf)  # Censored → upper bound is infinity\n",
    "\n",
    "print(\"AFT Label Bounds:\")\n",
    "print(f\"  Events: lower=upper (exact time)\")\n",
    "print(f\"  Censored: lower=time, upper=inf (right censored)\")\n",
    "print(f\"\\n  Events: {(y_upper != np.inf).sum()}\")\n",
    "print(f\"  Censored: {(y_upper == np.inf).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metric\n",
    "\n",
    "Use `concordance_index_ipcw` from scikit-survival which:\n",
    "- Handles censoring via Inverse Probability of Censoring Weighting (IPCW)\n",
    "- Uses a time truncation parameter τ (tau=7.0 years)\n",
    "\n",
    "Weighted formula: 0.3 × overall + 0.4 × test_like + 0.3 × high_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_cindex_ipcw(risk, y_surv_all, risk_groups, tau=7.0):\n",
    "    \"\"\"\n",
    "    Compute weighted C-index using concordance_index_ipcw (competition metric).\n",
    "    \n",
    "    Args:\n",
    "        risk: Risk scores (higher = worse prognosis)\n",
    "        y_surv_all: Structured survival array (event, time)\n",
    "        risk_groups: Dict with 'test_like' and 'high_risk' boolean masks\n",
    "        tau: Time truncation for IPCW\n",
    "    \n",
    "    Returns:\n",
    "        Dict with overall, test_like, high_risk, and weighted C-indices\n",
    "    \"\"\"\n",
    "    # Overall C-index\n",
    "    c_overall = concordance_index_ipcw(y_surv_all, y_surv_all, risk, tau=tau)[0]\n",
    "\n",
    "    # Test-like subgroup\n",
    "    mask_test = risk_groups['test_like'].values\n",
    "    y_surv_test = Surv.from_arrays(event=y_surv_all['event'][mask_test], time=y_surv_all['time'][mask_test])\n",
    "    c_test = concordance_index_ipcw(y_surv_all, y_surv_test, risk[mask_test], tau=tau)[0]\n",
    "\n",
    "    # High-risk subgroup\n",
    "    mask_high = risk_groups['high_risk'].values\n",
    "    y_surv_high = Surv.from_arrays(event=y_surv_all['event'][mask_high], time=y_surv_all['time'][mask_high])\n",
    "    c_high = concordance_index_ipcw(y_surv_all, y_surv_high, risk[mask_high], tau=tau)[0]\n",
    "\n",
    "    weighted = 0.3 * c_overall + 0.4 * c_test + 0.3 * c_high\n",
    "\n",
    "    return {\n",
    "        'overall': c_overall,\n",
    "        'test_like': c_test,\n",
    "        'high_risk': c_high,\n",
    "        'weighted': weighted\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost AFT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb_aft(X_tr, y_lower_tr, y_upper_tr, params, n_estimators):\n",
    "    \"\"\"\n",
    "    Train XGBoost AFT model.\n",
    "    \n",
    "    Args:\n",
    "        X_tr: Training features\n",
    "        y_lower_tr: Lower bound of survival time\n",
    "        y_upper_tr: Upper bound (inf for censored)\n",
    "        params: XGBoost parameters\n",
    "        n_estimators: Number of boosting rounds\n",
    "    \n",
    "    Returns:\n",
    "        Trained XGBoost model\n",
    "    \"\"\"\n",
    "    dtrain = xgb.DMatrix(X_tr)\n",
    "    dtrain.set_float_info('label_lower_bound', y_lower_tr) # Minimum possible event time\n",
    "    dtrain.set_float_info('label_upper_bound', y_upper_tr) # Maximum possible event time\n",
    "    \n",
    "    model = xgb.train(params, dtrain, num_boost_round=n_estimators, verbose_eval=False)\n",
    "    return model\n",
    "\n",
    "def predict_xgb_aft(model, X):\n",
    "    \"\"\"Predict risk score (negative predicted survival time).\"\"\"\n",
    "    dtest = xgb.DMatrix(X)\n",
    "    pred_time = model.predict(dtest)\n",
    "    # Higher predicted time = lower risk, so negate\n",
    "    return -pred_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Global OOF Evaluation\n",
    "\n",
    "**Global OOF (Out-Of-Fold)** evaluation:\n",
    "1. Split data into K folds\n",
    "2. For each fold, train on K-1 folds, predict on held-out fold\n",
    "3. Collect all OOF predictions\n",
    "4. Compute **single** C-index on all 3120 OOF predictions\n",
    "\n",
    "This is more robust than averaging per-fold C-indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_oof_evaluate(params, n_splits=5, seed=42):\n",
    "    \"\"\"Global OOF Cross-validation evaluation for XGBoost AFT.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof_preds = np.zeros(n_samples)\n",
    "    \n",
    "    X_arr = X_train.values\n",
    "    n_estimators = params.get('n_estimators', 100)\n",
    "    \n",
    "    xgb_params = {\n",
    "        'objective': 'survival:aft',\n",
    "        'eval_metric': 'aft-nloglik',\n",
    "        'aft_loss_distribution': params['aft_distribution'],\n",
    "        'aft_loss_distribution_scale': 1.0,\n",
    "        'tree_method': 'hist',\n",
    "        'max_depth': params['max_depth'],\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'min_child_weight': params['min_child_weight'],\n",
    "        'subsample': params['subsample'],\n",
    "        'colsample_bytree': params['colsample_bytree'],\n",
    "        'gamma': params['gamma'],\n",
    "        'reg_alpha': params['reg_alpha'],\n",
    "        'reg_lambda': params['reg_lambda'],\n",
    "        'seed': seed,\n",
    "    }\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_arr, strat_var)):\n",
    "        X_tr, X_val = X_arr[train_idx], X_arr[val_idx]\n",
    "        y_lower_tr = y_lower[train_idx]\n",
    "        y_upper_tr = y_upper[train_idx]\n",
    "        \n",
    "        xgb_params['seed'] = seed + fold_idx\n",
    "        model = train_xgb_aft(X_tr, y_lower_tr, y_upper_tr, xgb_params, n_estimators)\n",
    "        oof_preds[val_idx] = predict_xgb_aft(model, X_val)\n",
    "    \n",
    "    # Global Z-score normalization\n",
    "    oof_normalized = (oof_preds - oof_preds.mean()) / (oof_preds.std() + 1e-8)\n",
    "    \n",
    "    # Compute metrics using competition metric\n",
    "    return weighted_cindex_ipcw(oof_normalized, y_surv, risk_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Optuna hyperparameter tuning...\n",
      "(Set n_trials=100 for full tuning)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ece1faf5413480cb35d886037002ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best trial:\n",
      "  Weighted C-index: 0.6964\n",
      "  Params: {'aft_distribution': 'normal', 'n_estimators': 147, 'max_depth': 5, 'learning_rate': 0.026341881840794876, 'min_child_weight': 41, 'subsample': 0.9204349911732258, 'colsample_bytree': 0.5136953028400747, 'gamma': 2.8297130955076955, 'reg_alpha': 0.09570260464777246, 'reg_lambda': 0.4468062524655007}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Optuna objective for XGBoost AFT hyperparameter tuning.\"\"\"\n",
    "    params = {\n",
    "        'aft_distribution': trial.suggest_categorical('aft_distribution', ['normal', 'logistic', 'extreme']),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 10, 200),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10, log=True),\n",
    "    }\n",
    "    \n",
    "    result = global_oof_evaluate(params)\n",
    "    return result['weighted']\n",
    "\n",
    "# Run Optuna study (reduced trials for notebook)\n",
    "print(\"Running Optuna hyperparameter tuning...\")\n",
    "print(\"(Set n_trials=100 for full tuning)\\n\")\n",
    "\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True)  # Use 100 for full tuning\n",
    "\n",
    "print(f\"\\nBest trial:\")\n",
    "print(f\"  Weighted C-index: {study.best_value:.4f}\")\n",
    "print(f\"  Params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Model Configuration\n",
    "\n",
    "From full 100-trial Optuna tuning with competition metric (`concordance_index_ipcw`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters (from 100-trial tuning, competition metric):\n",
      "  aft_distribution: normal\n",
      "  n_estimators: 147\n",
      "  max_depth: 5\n",
      "  learning_rate: 0.026342\n",
      "  min_child_weight: 41\n",
      "  subsample: 0.920435\n",
      "  colsample_bytree: 0.513695\n",
      "  gamma: 2.829713\n",
      "  reg_alpha: 0.095703\n",
      "  reg_lambda: 0.446806\n",
      "\n",
      "CV Results (concordance_index_ipcw):\n",
      "  Overall C-index: 0.7214\n",
      "  Test-like C-index: 0.6967\n",
      "  High-risk C-index: 0.6709\n",
      "  Weighted C-index: 0.6964\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters from full tuning (competition metric)\n",
    "BEST_PARAMS = {\n",
    "    'aft_distribution': 'normal',\n",
    "    'n_estimators': 147,\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.026342,\n",
    "    'min_child_weight': 41,\n",
    "    'subsample': 0.920435,\n",
    "    'colsample_bytree': 0.513695,\n",
    "    'gamma': 2.829713,\n",
    "    'reg_alpha': 0.095703,\n",
    "    'reg_lambda': 0.446806,\n",
    "}\n",
    "\n",
    "print(\"Best Hyperparameters (from 100-trial tuning, competition metric):\")\n",
    "for k, v in BEST_PARAMS.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Evaluate with best params\n",
    "result = global_oof_evaluate(BEST_PARAMS)\n",
    "print(f\"\\nCV Results (concordance_index_ipcw):\")\n",
    "print(f\"  Overall C-index: {result['overall']:.4f}\")\n",
    "print(f\"  Test-like C-index: {result['test_like']:.4f}\")\n",
    "print(f\"  High-risk C-index: {result['high_risk']:.4f}\")\n",
    "print(f\"  Weighted C-index: {result['weighted']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Final Model and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model on full data...\n",
      "Model trained with 147 trees.\n"
     ]
    }
   ],
   "source": [
    "# Train on full data\n",
    "print(\"Training final model on full data...\")\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'survival:aft',\n",
    "    'eval_metric': 'aft-nloglik',\n",
    "    'aft_loss_distribution': BEST_PARAMS['aft_distribution'],\n",
    "    'aft_loss_distribution_scale': 1.0,\n",
    "    'tree_method': 'hist',\n",
    "    'max_depth': BEST_PARAMS['max_depth'],\n",
    "    'learning_rate': BEST_PARAMS['learning_rate'],\n",
    "    'min_child_weight': BEST_PARAMS['min_child_weight'],\n",
    "    'subsample': BEST_PARAMS['subsample'],\n",
    "    'colsample_bytree': BEST_PARAMS['colsample_bytree'],\n",
    "    'gamma': BEST_PARAMS['gamma'],\n",
    "    'reg_alpha': BEST_PARAMS['reg_alpha'],\n",
    "    'reg_lambda': BEST_PARAMS['reg_lambda'],\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "final_model = train_xgb_aft(\n",
    "    X_train.values, y_lower, y_upper,\n",
    "    xgb_params, BEST_PARAMS['n_estimators']\n",
    ")\n",
    "\n",
    "print(f\"Model trained with {BEST_PARAMS['n_estimators']} trees.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Features by Gain:\n",
      "   feature  importance\n",
      "13     f16  102.165848\n",
      "45     f80   73.549072\n",
      "11     f14   68.856689\n",
      "24     f28   41.682056\n",
      "37     f72   29.986715\n",
      "0       f0   28.927471\n",
      "10     f13   24.852905\n",
      "23     f27   23.491613\n",
      "43     f78   20.367319\n",
      "3       f3   19.378695\n",
      "4       f4   17.108749\n",
      "7      f10   16.551325\n",
      "9      f12   14.190485\n",
      "41     f76   13.537127\n",
      "44     f79   13.203796\n",
      "19     f23   12.672067\n",
      "25     f29   12.590256\n",
      "40     f75   12.580143\n",
      "47     f82   12.473783\n",
      "15     f19   12.048156\n"
     ]
    }
   ],
   "source": [
    "# Feature importance\n",
    "importance = final_model.get_score(importance_type='gain')\n",
    "importance_df = pd.DataFrame([\n",
    "    {'feature': k, 'importance': v} for k, v in importance.items()\n",
    "]).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Features by Gain:\")\n",
    "print(importance_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test predictions:\n",
      "  Samples: 1193\n",
      "  Risk range: [-11.5427, -0.1924]\n"
     ]
    }
   ],
   "source": [
    "# Load test data and generate predictions\n",
    "X_test_with_id = pd.read_csv(f'{TRAIN_PATH}/X_test_83features_with_id.csv')\n",
    "test_ids = X_test_with_id['ID'].values\n",
    "X_test = X_test_with_id.drop(columns=['ID']).values\n",
    "\n",
    "# Predict\n",
    "test_risk = predict_xgb_aft(final_model, X_test)\n",
    "\n",
    "print(f\"\\nTest predictions:\")\n",
    "print(f\"  Samples: {len(test_risk)}\")\n",
    "print(f\"  Risk range: [{test_risk.min():.4f}, {test_risk.max():.4f}]\")\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'risk_score': test_risk\n",
    "})\n",
    "# Save to outputs/submissions/\n",
    "import os\n",
    "os.makedirs('outputs/submissions', exist_ok=True)\n",
    "output_path = 'outputs/submissions/submission_xgb_aft.csv'\n",
    "submission.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### XGBoost AFT Model Results\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Overall C-index | 0.7214 |\n",
    "| Test-like C-index | 0.6967 |\n",
    "| High-risk C-index | 0.6709 |\n",
    "| **Weighted C-index** | **0.6964** |\n",
    "\n",
    "### Key Findings\n",
    "1. **Best single model** in our experiments\n",
    "2. Normal (log-normal) distribution works best\n",
    "3. Handles NaN natively - no imputation needed\n",
    "4. 83 unfixed features outperform 128 fixed features\n",
    "\n",
    "Public leaderboard score: 0.7534"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
