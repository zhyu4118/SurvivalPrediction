{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 7: Ensembling Strategies\n",
    "\n",
    "We use:\n",
    "- Simple weighted average of 4 models (normalized scores) for complementary signals, reduced variance, and robust prediction (less sensitive to hyperparameter choices). \n",
    "- Weight grid search for optimal combination\n",
    "\n",
    "## Best Ensemble Configuration\n",
    "- **XGB AFT (83 unfixed)**: 50%\n",
    "- **CoxPH elastic net (128 fixed)**: 5%\n",
    "- **DeepSurv (83 fixed)**: 15%\n",
    "- **CatBoost CLF + LGB REG (128 fixed)**: 30%\n",
    "- **CV Score**: 0.6989 weighted C-index (+0.0025 over best single model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "from lifelines import KaplanMeierFitter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "from sksurv.util import Surv\n",
    "\n",
    "TRAIN_PATH = '/your_path/SurvivalPrediction/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 3120\n",
      "X_83_unfixed: (3120, 83), X_83_fixed_scaled: (3120, 83)\n",
      "X_128_fixed: (3120, 128), X_128_fixed_scaled: (3120, 128)\n"
     ]
    }
   ],
   "source": [
    "# Load all datasets\n",
    "X_83_unfixed_full = pd.read_csv(f'{TRAIN_PATH}/X_train_83features_with_id.csv')\n",
    "X_83_fixed_scaled_full = pd.read_csv(f'{TRAIN_PATH}/X_train_83features_with_id_fixed_scaled.csv')\n",
    "X_128_fixed_full = pd.read_csv(f'{TRAIN_PATH}/X_train_128features_with_id_clean_fixed.csv')\n",
    "X_128_fixed_scaled_full = pd.read_csv(f'{TRAIN_PATH}/X_train_128features_with_id_clean_fixed_scaled.csv')\n",
    "target = pd.read_csv(f'{TRAIN_PATH}/target_train_clean_aligned.csv')\n",
    "\n",
    "# Align all datasets with target (filter + reorder to match target's patient sequence)\n",
    "X_83_unfixed = X_83_unfixed_full.set_index('ID').loc[target['ID']].reset_index(drop=True)\n",
    "X_83_fixed_scaled = X_83_fixed_scaled_full.set_index('ID').loc[target['ID']].reset_index(drop=True)\n",
    "X_128_fixed = X_128_fixed_full.set_index('ID').loc[target['ID']].reset_index(drop=True)\n",
    "X_128_fixed_scaled = X_128_fixed_scaled_full.set_index('ID').loc[target['ID']].reset_index(drop=True)\n",
    "\n",
    "y_time = target['OS_YEARS'].values\n",
    "y_event = target['OS_STATUS'].values.astype(bool)\n",
    "y_surv = Surv.from_arrays(event=y_event, time=y_time)\n",
    "y_lower = y_time.copy()\n",
    "y_upper = np.where(y_event, y_time, np.inf)\n",
    "n_samples = len(target)\n",
    "\n",
    "# Risk groups\n",
    "def define_risk_groups(X):\n",
    "    risk_factors = pd.DataFrame(index=X.index)\n",
    "    risk_factors['high_blast'] = (X['BM_BLAST'] > 10).astype(int)\n",
    "    risk_factors['has_TP53'] = (X['has_TP53'] > 0).astype(int)\n",
    "    risk_factors['low_hb'] = (X['HB'] < 10).astype(int)\n",
    "    risk_factors['low_plt'] = (X['PLT'] < 50).astype(int)\n",
    "    risk_factors['high_cyto'] = (X['cyto_risk_score'] >= 3).astype(int)\n",
    "    n_risk_factors = risk_factors.sum(axis=1)\n",
    "    return {'test_like': n_risk_factors >= 1, 'high_risk': n_risk_factors >= 2}\n",
    "\n",
    "risk_groups = define_risk_groups(X_83_unfixed)\n",
    "has_tp53 = (X_83_unfixed['has_TP53'] > 0).astype(int).values\n",
    "strat_var = pd.Series([f\"{int(e)}_{int(t)}\" for e, t in zip(y_event, has_tp53)])\n",
    "\n",
    "print(f\"Samples: {n_samples}\")\n",
    "print(f\"X_83_unfixed: {X_83_unfixed.shape}, X_83_fixed_scaled: {X_83_fixed_scaled.shape}\")\n",
    "print(f\"X_128_fixed: {X_128_fixed.shape}, X_128_fixed_scaled: {X_128_fixed_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definitions and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters loaded for all 4 models.\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metric\n",
    "def weighted_cindex_ipcw(risk, y_surv_all, risk_groups, tau=7.0):\n",
    "    c_overall = concordance_index_ipcw(y_surv_all, y_surv_all, risk, tau=tau)[0]\n",
    "\n",
    "    mask_test = risk_groups['test_like'].values\n",
    "    y_surv_test = Surv.from_arrays(event=y_surv_all['event'][mask_test], time=y_surv_all['time'][mask_test])\n",
    "    c_test = concordance_index_ipcw(y_surv_all, y_surv_test, risk[mask_test], tau=tau)[0]\n",
    "\n",
    "    mask_high = risk_groups['high_risk'].values\n",
    "    y_surv_high = Surv.from_arrays(event=y_surv_all['event'][mask_high], time=y_surv_all['time'][mask_high])\n",
    "    c_high = concordance_index_ipcw(y_surv_all, y_surv_high, risk[mask_high], tau=tau)[0]\n",
    "\n",
    "    weighted = 0.3 * c_overall + 0.4 * c_test + 0.3 * c_high\n",
    "    return {'overall': c_overall, 'test_like': c_test, 'high_risk': c_high, 'weighted': weighted}\n",
    "\n",
    "# Best hyperparameters from individual model tuning\n",
    "xgb_aft_params = {\n",
    "    'n_estimators': 147, 'max_depth': 5, 'learning_rate': 0.026342,\n",
    "    'min_child_weight': 41, 'subsample': 0.920435, 'colsample_bytree': 0.513695,\n",
    "    'gamma': 2.829713, 'reg_alpha': 0.095703, 'reg_lambda': 0.446806,\n",
    "}\n",
    "xgb_aft_dist = 'normal'\n",
    "\n",
    "coxph_l1_ratio = 0.4380\n",
    "coxph_alpha = 0.0391\n",
    "\n",
    "deepsurv_params = {\n",
    "    'hidden_layers': [64, 64, 64], 'dropout': 0.6594556222699043, 'activation': 'selu',\n",
    "    'lr': 0.00011237410700529054, 'batch_size': 32, 'epochs': 58,\n",
    "}\n",
    "\n",
    "two_model_params = {'cat_depth': 5, 'cat_iterations': 262, \n",
    "                    'cat_learning_rate': 0.019227204273246305, 'cat_l2_leaf_reg': 0.12876998314647772, \n",
    "                    'lgb_max_depth': 9, 'lgb_n_estimators': 149, 'lgb_learning_rate': 0.08025255147825751, \n",
    "                    'lgb_num_leaves': 36, 'lgb_min_child_samples': 10, 'lgb_subsample': 0.9638486108112962, \n",
    "                    'lgb_colsample_bytree': 0.8126162431458441, 'lgb_reg_alpha': 4.6796458896222854e-06, \n",
    "                    'lgb_reg_lambda': 4.190436671160808e-07\n",
    "                    }\n",
    "\n",
    "print(\"Hyperparameters loaded for all 4 models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preliminary Functions\n",
    "\n",
    "DeepSurv neural network with custom-defined loss, KM sample weights, and merge function for the Two-Model approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model definitions loaded: DeepSurvNet, cox_ph_loss, compute_sample_weights, merge_predictions\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "# Define neural network and loss function; same as in 05_deepsurv_cox.ipynb\n",
    "class DeepSurvNet(nn.Module):\n",
    "    def __init__(self, in_features, hidden_layers, dropout=0.1, activation='relu'):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_size = in_features\n",
    "        act_fn = nn.ReLU() if activation == 'relu' else nn.SELU()\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(act_fn)\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def cox_ph_loss(risk_pred, time, event):\n",
    "    \"\"\"Cox partial likelihood loss.\"\"\"\n",
    "    # sort by survival time (descending)\n",
    "    sorted_indices = torch.argsort(time, descending=True) \n",
    "    sorted_risk = risk_pred[sorted_indices]\n",
    "    sorted_event = event[sorted_indices]\n",
    "\n",
    "    # compute risk set denominators log(sum of exp)\n",
    "    log_risk = torch.logcumsumexp(sorted_risk, dim=0)\n",
    "\n",
    "    # partial likelihood contribution \n",
    "    uncensored_likelihood = sorted_risk - log_risk\n",
    "\n",
    "    # mask out censored patients\n",
    "    censored_likelihood = uncensored_likelihood * sorted_event\n",
    "\n",
    "    # negative log partial likelihood\n",
    "    return -torch.sum(censored_likelihood)\n",
    "\n",
    "# Same functions as in 06_two_model_approach.ipynb\n",
    "# Sample weight computation\n",
    "def compute_sample_weights(times, events):\n",
    "    \"\"\"\n",
    "    Events (deaths): weight = 1.0\n",
    "    Censored: weight = F(t) / F_max (KM cumulative density)\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit Kaplan_Meier curve\n",
    "    kmf_event = KaplanMeierFitter()\n",
    "    kmf_event.fit(times, event_observed=events)\n",
    "    \n",
    "    # Get maximum cdf\n",
    "    t_max = times.max() # maximum observed time\n",
    "    F_max = kmf_event.cumulative_density_at_times([t_max]).values[0] #maximum cdf value\n",
    "    F_max = max(F_max, 0.01) # clipping to avoid division by zero\n",
    "\n",
    "    # Assign weights to samples\n",
    "    weights = np.zeros(len(times))\n",
    "    for i in range(len(times)):\n",
    "        if events[i] == 1:\n",
    "            weights[i] = 1.0\n",
    "        else:\n",
    "            F_t = kmf_event.cumulative_density_at_times([times[i]]).values[0]\n",
    "            weights[i] = F_t / F_max\n",
    "            \n",
    "    # normalize so average weight = 1.0\n",
    "    weights = weights / weights.mean()\n",
    "    return weights\n",
    "\n",
    "# Merge function: combine classifier and regressor predictions\n",
    "def merge_predictions(clf_pred, reg_pred, time_min, time_max):\n",
    "    # normalized predicted times then clip to [0,1]\n",
    "    pred_time_norm = (reg_pred - time_min) / (time_max - time_min + 1e-8)\n",
    "    pred_time_norm = np.clip(pred_time_norm, 0, 1) \n",
    "\n",
    "    # compute odds of predicted population deaths\n",
    "    avg_pred_event = np.mean(clf_pred)\n",
    "    odds = avg_pred_event / (1 - avg_pred_event + 1e-8)\n",
    "    odds = np.clip(odds, 0.1, 10)\n",
    "\n",
    "    # compute score\n",
    "    risk = clf_pred * (1 + odds * (1 - pred_time_norm))\n",
    "    return risk\n",
    "\n",
    "print(\"Model definitions loaded: DeepSurvNet, cox_ph_loss, compute_sample_weights, merge_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate OOF Predictions\n",
    "\n",
    "Train all 4 models in 5-fold CV, collecting out-of-fold predictions for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5-fold CV for all 4 models...\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "  XGB AFT... done\n",
      "  CoxPH... done\n",
      "  DeepSurv... done\n",
      "  CatBoost CLF + LGB REG... done\n",
      "--- Fold 2/5 ---\n",
      "  XGB AFT... done\n",
      "  CoxPH... done\n",
      "  DeepSurv... done\n",
      "  CatBoost CLF + LGB REG... done\n",
      "--- Fold 3/5 ---\n",
      "  XGB AFT... done\n",
      "  CoxPH... done\n",
      "  DeepSurv... done\n",
      "  CatBoost CLF + LGB REG... done\n",
      "--- Fold 4/5 ---\n",
      "  XGB AFT... done\n",
      "  CoxPH... done\n",
      "  DeepSurv... done\n",
      "  CatBoost CLF + LGB REG... done\n",
      "--- Fold 5/5 ---\n",
      "  XGB AFT... done\n",
      "  CoxPH... done\n",
      "  DeepSurv... done\n",
      "  CatBoost CLF + LGB REG... done\n",
      "\n",
      "OOF predictions generated for all 4 models!\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "seed = 42\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "oof_xgb_aft = np.zeros(n_samples)\n",
    "oof_coxph = np.zeros(n_samples)\n",
    "oof_deepsurv = np.zeros(n_samples)\n",
    "oof_twomodel = np.zeros(n_samples)\n",
    "\n",
    "print(\"Running 5-fold CV for all 4 models...\\n\")\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_83_unfixed, strat_var)):\n",
    "    print(f\"--- Fold {fold_idx+1}/{n_splits} ---\")\n",
    "\n",
    "    # Model 1: XGB AFT 83 unfixed\n",
    "    print(\"  XGB AFT...\", end=\" \")\n",
    "    X_tr = X_83_unfixed.iloc[train_idx].values\n",
    "    X_val = X_83_unfixed.iloc[val_idx].values\n",
    "    dtrain = xgb.DMatrix(X_tr)\n",
    "    dtrain.set_float_info('label_lower_bound', y_lower[train_idx])\n",
    "    dtrain.set_float_info('label_upper_bound', y_upper[train_idx])\n",
    "    dval = xgb.DMatrix(X_val)\n",
    "    params = {\n",
    "        'objective': 'survival:aft', 'eval_metric': 'aft-nloglik',\n",
    "        'aft_loss_distribution': xgb_aft_dist, 'aft_loss_distribution_scale': 1.0,\n",
    "        'tree_method': 'hist', 'seed': seed + fold_idx,\n",
    "        **{k: v for k, v in xgb_aft_params.items() if k != 'n_estimators'},\n",
    "    }\n",
    "    model = xgb.train(params, dtrain, num_boost_round=xgb_aft_params['n_estimators'], verbose_eval=False)\n",
    "    oof_xgb_aft[val_idx] = -model.predict(dval)\n",
    "    print(\"done\")\n",
    "\n",
    "    # Model 2: CoxPH 128 fixed (elastic net)\n",
    "    print(\"  CoxPH...\", end=\" \")\n",
    "    X_tr_128 = X_128_fixed.iloc[train_idx].values\n",
    "    X_val_128 = X_128_fixed.iloc[val_idx].values\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_128_scaled = scaler.fit_transform(X_tr_128)\n",
    "    X_val_128_scaled = scaler.transform(X_val_128)\n",
    "    y_surv_tr = Surv.from_arrays(event=y_event[train_idx], time=y_time[train_idx])\n",
    "    coxph = CoxnetSurvivalAnalysis(l1_ratio=coxph_l1_ratio, alphas=[coxph_alpha])\n",
    "    coxph.fit(X_tr_128_scaled, y_surv_tr)\n",
    "    oof_coxph[val_idx] = coxph.predict(X_val_128_scaled)\n",
    "    print(\"done\")\n",
    "\n",
    "    # Model 3: DeepSurv 83 fixed\n",
    "    print(\"  DeepSurv...\", end=\" \")\n",
    "    X_tr_ds = X_83_fixed_scaled.iloc[train_idx].values\n",
    "    X_val_ds = X_83_fixed_scaled.iloc[val_idx].values\n",
    "    torch.manual_seed(seed + fold_idx)\n",
    "    np.random.seed(seed + fold_idx)\n",
    "    ds_model = DeepSurvNet(X_tr_ds.shape[1], deepsurv_params['hidden_layers'],\n",
    "                           deepsurv_params['dropout'], deepsurv_params['activation']).to(device)\n",
    "    optimizer = torch.optim.Adam(ds_model.parameters(), lr=deepsurv_params['lr'], weight_decay=1e-4)\n",
    "    X_tensor = torch.FloatTensor(X_tr_ds).to(device)\n",
    "    time_tensor = torch.FloatTensor(y_time[train_idx]).to(device)\n",
    "    event_tensor = torch.FloatTensor(y_event[train_idx].astype(float)).to(device)\n",
    "    batch_size = deepsurv_params['batch_size']\n",
    "    n_train = len(X_tr_ds)\n",
    "    for epoch in range(deepsurv_params['epochs']):\n",
    "        ds_model.train()\n",
    "        indices = np.random.permutation(n_train)\n",
    "        for start in range(0, n_train, batch_size):\n",
    "            end = min(start + batch_size, n_train)\n",
    "            batch_idx = indices[start:end]\n",
    "            if len(batch_idx) < 10:\n",
    "                continue\n",
    "            optimizer.zero_grad()\n",
    "            risk_pred = ds_model(X_tensor[batch_idx]).squeeze()\n",
    "            loss = cox_ph_loss(risk_pred, time_tensor[batch_idx], event_tensor[batch_idx])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    ds_model.eval()\n",
    "    with torch.no_grad():\n",
    "        oof_deepsurv[val_idx] = ds_model(torch.FloatTensor(X_val_ds).to(device)).squeeze().cpu().numpy()\n",
    "    print(\"done\")\n",
    "\n",
    "    # Model 4: CatBoost CLF + LightGBM REG (4th place sample weights)\n",
    "    print(\"  CatBoost CLF + LGB REG...\", end=\" \")\n",
    "    X_tr_tm = X_128_fixed_scaled.iloc[train_idx].values\n",
    "    X_val_tm = X_128_fixed_scaled.iloc[val_idx].values\n",
    "    y_time_tr = y_time[train_idx]\n",
    "    y_event_tr = y_event[train_idx].astype(int)\n",
    "    clf_weights = compute_sample_weights(y_time_tr, y_event_tr)\n",
    "    cat_clf = CatBoostClassifier(\n",
    "        depth=int(two_model_params.get('cat_depth', 5)),\n",
    "        iterations=int(two_model_params.get('cat_iterations', 262)),\n",
    "        learning_rate=two_model_params.get('cat_learning_rate', 0.019227204273246305),\n",
    "        l2_leaf_reg=two_model_params.get('cat_l2_leaf_reg', 0.12876998314647772),\n",
    "        random_seed=seed + fold_idx, verbose=False, allow_writing_files=False,\n",
    "    )\n",
    "    cat_clf.fit(X_tr_tm, y_event_tr, sample_weight=clf_weights)\n",
    "    clf_pred = cat_clf.predict_proba(X_val_tm)[:, 1]\n",
    "    event_mask = y_event_tr == 1\n",
    "    lgb_reg = lgb.LGBMRegressor(\n",
    "        max_depth=int(two_model_params.get('lgb_max_depth', 9)),\n",
    "        n_estimators=int(two_model_params.get('lgb_n_estimators', 149)),\n",
    "        learning_rate=two_model_params.get('lgb_learning_rate', 0.08025255147825751),\n",
    "        num_leaves=int(two_model_params.get('lgb_num_leaves', 36)),\n",
    "        min_child_samples=int(two_model_params.get('lgb_min_child_samples', 10)),\n",
    "        subsample=two_model_params.get('lgb_subsample', 0.9638486108112962),\n",
    "        colsample_bytree=two_model_params.get('lgb_colsample_bytree', 0.8126162431458441),\n",
    "        reg_alpha=two_model_params.get('lgb_reg_alpha', 4.6796458896222854e-06),\n",
    "        reg_lambda=two_model_params.get('lgb_reg_lambda', 4.190436671160808e-07),\n",
    "        random_state=seed + fold_idx, verbosity=-1,\n",
    "    )\n",
    "    lgb_reg.fit(X_tr_tm[event_mask], y_time_tr[event_mask])\n",
    "    reg_pred = lgb_reg.predict(X_val_tm)\n",
    "    oof_twomodel[val_idx] = merge_predictions(clf_pred, reg_pred, y_time_tr.min(), y_time_tr.max())\n",
    "    print(\"done\")\n",
    "\n",
    "print(\"\\nOOF predictions generated for all 4 models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Normalize and Evaluate Individual Models\n",
    "\n",
    "Z-score normalization ensures all models contribute on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Model Performance (OOF, competition metric):\n",
      "Model                           Overall  Test-like  High-risk  Weighted\n",
      "----------------------------------------------------------------------\n",
      "XGB AFT 83 unfixed               0.7214     0.6967     0.6709    0.6964\n",
      "CoxPH 128 fixed                  0.7175     0.6931     0.6607    0.6907\n",
      "DeepSurv 83 fixed                0.7169     0.6918     0.6614    0.6902\n",
      "CatBoost CLF + LGB REG           0.7184     0.6937     0.6608    0.6912\n"
     ]
    }
   ],
   "source": [
    "# Z-score normalize OOF predictions\n",
    "oof_xgb_aft_norm = (oof_xgb_aft - oof_xgb_aft.mean()) / (oof_xgb_aft.std() + 1e-8)\n",
    "oof_coxph_norm = (oof_coxph - oof_coxph.mean()) / (oof_coxph.std() + 1e-8)\n",
    "oof_deepsurv_norm = (oof_deepsurv - oof_deepsurv.mean()) / (oof_deepsurv.std() + 1e-8)\n",
    "oof_twomodel_norm = (oof_twomodel - oof_twomodel.mean()) / (oof_twomodel.std() + 1e-8)\n",
    "\n",
    "# Evaluate individual models\n",
    "print(\"Individual Model Performance (OOF, competition metric):\")\n",
    "print(f\"{'Model':<30} {'Overall':>8} {'Test-like':>10} {'High-risk':>10} {'Weighted':>9}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, preds in [('XGB AFT 83 unfixed', oof_xgb_aft_norm),\n",
    "                     ('CoxPH 128 fixed', oof_coxph_norm),\n",
    "                     ('DeepSurv 83 fixed', oof_deepsurv_norm),\n",
    "                     ('CatBoost CLF + LGB REG', oof_twomodel_norm)]:\n",
    "    r = weighted_cindex_ipcw(preds, y_surv, risk_groups)\n",
    "    print(f\"{name:<30} {r['overall']:>8.4f} {r['test_like']:>10.4f} {r['high_risk']:>10.4f} {r['weighted']:>9.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Weight Grid Search\n",
    "\n",
    "Search over all valid 4-weight combinations (step=0.05, 1,771 total) to find the optimal ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid step: 0.05\n",
      "Total valid weight combinations: 1771\n",
      "Searching...\n",
      "\n",
      "Best weights: XGB=0.60, Cox=0.00, DS=0.20, TM=0.20\n",
      "Best weighted C-index: 0.6999\n",
      "\n",
      "Top 10 Weight Combinations:\n",
      "   XGB    Cox     DS     TM | Weighted\n",
      "------------------------------------------\n",
      "  0.60   0.00   0.20   0.20 |   0.6999\n",
      "  0.55   0.05   0.15   0.25 |   0.6998\n",
      "  0.55   0.00   0.20   0.25 |   0.6998\n",
      "  0.55   0.05   0.20   0.20 |   0.6998\n",
      "  0.65   0.00   0.20   0.15 |   0.6998\n",
      "  0.60   0.05   0.15   0.20 |   0.6998\n",
      "  0.50   0.05   0.15   0.30 |   0.6998\n",
      "  0.55   0.00   0.25   0.20 |   0.6998\n",
      "  0.50   0.00   0.25   0.25 |   0.6998\n",
      "  0.50   0.05   0.20   0.25 |   0.6997\n",
      "\n",
      "Improvement over best single model: +0.0035\n"
     ]
    }
   ],
   "source": [
    "grid_step = 0.05\n",
    "grid_values = np.arange(0, 1 + grid_step/2, grid_step)\n",
    "\n",
    "# Generate all valid weight combinations (sum = 1)\n",
    "valid_combinations = []\n",
    "for w1 in grid_values:\n",
    "    for w2 in grid_values:\n",
    "        for w3 in grid_values:\n",
    "            w4 = 1 - w1 - w2 - w3\n",
    "            if w4 >= -1e-9 and w4 <= 1 + 1e-9:\n",
    "                valid_combinations.append((w1, w2, w3, max(0, min(1, w4))))\n",
    "\n",
    "print(f\"Grid step: {grid_step}\")\n",
    "print(f\"Total valid weight combinations: {len(valid_combinations)}\")\n",
    "\n",
    "\n",
    "best_weighted = 0\n",
    "best_weights = None\n",
    "best_results = None\n",
    "all_results = []\n",
    "\n",
    "print(\"Searching...\")\n",
    "for w1, w2, w3, w4 in valid_combinations:\n",
    "    ensemble_pred = (w1 * oof_xgb_aft_norm + w2 * oof_coxph_norm +\n",
    "                     w3 * oof_deepsurv_norm + w4 * oof_twomodel_norm)\n",
    "    results = weighted_cindex_ipcw(ensemble_pred, y_surv, risk_groups)\n",
    "    all_results.append({\n",
    "        'w_xgb_aft': w1, 'w_coxph': w2, 'w_deepsurv': w3, 'w_twomodel': w4,\n",
    "        'overall': results['overall'], 'test_like': results['test_like'],\n",
    "        'high_risk': results['high_risk'], 'weighted': results['weighted'],\n",
    "    })\n",
    "    if results['weighted'] > best_weighted:\n",
    "        best_weighted = results['weighted']\n",
    "        best_weights = (w1, w2, w3, w4)\n",
    "        best_results = results\n",
    "\n",
    "print(f\"\\nBest weights: XGB={best_weights[0]:.2f}, Cox={best_weights[1]:.2f}, \"\n",
    "      f\"DS={best_weights[2]:.2f}, TM={best_weights[3]:.2f}\")\n",
    "print(f\"Best weighted C-index: {best_results['weighted']:.4f}\")\n",
    "\n",
    "# Top 10 combinations\n",
    "results_df = pd.DataFrame(all_results).sort_values('weighted', ascending=False)\n",
    "print(f\"\\nTop 10 Weight Combinations:\")\n",
    "print(f\"{'XGB':>6} {'Cox':>6} {'DS':>6} {'TM':>6} | {'Weighted':>8}\")\n",
    "print(\"-\" * 42)\n",
    "for _, row in results_df.head(10).iterrows():\n",
    "    print(f\"{row['w_xgb_aft']:6.2f} {row['w_coxph']:6.2f} {row['w_deepsurv']:6.2f} \"\n",
    "          f\"{row['w_twomodel']:6.2f} | {row['weighted']:8.4f}\")\n",
    "\n",
    "# Comparison with best single model\n",
    "single_best = weighted_cindex_ipcw(oof_xgb_aft_norm, y_surv, risk_groups)\n",
    "print(f\"\\nImprovement over best single model: {best_results['weighted'] - single_best['weighted']:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Final Models and Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Test samples: 1193\n",
      "\n",
      "Training final models on full training data...\n",
      "  Training XGB AFT... done\n",
      "  Training CoxPH... done\n",
      "  Training DeepSurv... done\n",
      "  Training CatBoost CLF + LGB REG... done\n",
      "\n",
      "Normalizing test predictions...\n",
      "\n",
      "Ensemble weights: XGB=0.50, Cox=0.05, DS=0.15, TM=0.30\n",
      "\n",
      "Submission file created: outputs/submissions/submission_ensemble_4model.csv\n",
      "  Shape: (1193, 2)\n",
      "  Risk range: [-3.6665, 1.4591]\n",
      "\n",
      "First few predictions:\n",
      "      ID  risk_score\n",
      "0   KYW1    0.771414\n",
      "1   KYW2    0.851775\n",
      "2   KYW3    0.111809\n",
      "3   KYW4    0.757784\n",
      "4   KYW5    0.684432\n",
      "5   KYW6    0.440218\n",
      "6   KYW7    0.269404\n",
      "7   KYW8    0.404195\n",
      "8   KYW9   -1.593878\n",
      "9  KYW10   -1.093464\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "X_test_83_unfixed = pd.read_csv(f'{TRAIN_PATH}/X_test_83features_with_id.csv')\n",
    "X_test_83_fixed_scaled = pd.read_csv(f'{TRAIN_PATH}/X_test_83features_with_id_fixed_scaled.csv')\n",
    "X_test_128_fixed_scaled = pd.read_csv(f'{TRAIN_PATH}/X_test_128features_with_id_fixed_scaled.csv')\n",
    "\n",
    "# Extract IDs and remove from features\n",
    "test_ids = X_test_83_unfixed['ID'].values\n",
    "X_test_83_unfixed = X_test_83_unfixed.drop(columns=['ID'])\n",
    "X_test_83_fixed_scaled = X_test_83_fixed_scaled.drop(columns=['ID'])\n",
    "X_test_128_fixed_scaled = X_test_128_fixed_scaled.drop(columns=['ID'])\n",
    "\n",
    "print(f\"Test samples: {len(test_ids)}\")\n",
    "\n",
    "# Train final models on full training data\n",
    "print(\"\\nTraining final models on full training data...\")\n",
    "\n",
    "# Model 1: XGB AFT 83 unfixed\n",
    "print(\"  Training XGB AFT...\", end=\" \")\n",
    "X_train_full = X_83_unfixed.values\n",
    "dtrain_full = xgb.DMatrix(X_train_full)\n",
    "dtrain_full.set_float_info('label_lower_bound', y_lower)\n",
    "dtrain_full.set_float_info('label_upper_bound', y_upper)\n",
    "dtest = xgb.DMatrix(X_test_83_unfixed.values)\n",
    "params_xgb = {\n",
    "    'objective': 'survival:aft', 'eval_metric': 'aft-nloglik',\n",
    "    'aft_loss_distribution': xgb_aft_dist, 'aft_loss_distribution_scale': 1.0,\n",
    "    'tree_method': 'hist', 'seed': seed,\n",
    "    **{k: v for k, v in xgb_aft_params.items() if k != 'n_estimators'},\n",
    "}\n",
    "model_xgb = xgb.train(params_xgb, dtrain_full, num_boost_round=xgb_aft_params['n_estimators'], verbose_eval=False)\n",
    "test_pred_xgb_aft = -model_xgb.predict(dtest)\n",
    "print(\"done\")\n",
    "\n",
    "# Model 2: CoxPH 128 fixed\n",
    "print(\"  Training CoxPH...\", end=\" \")\n",
    "X_train_128_full = X_128_fixed.values\n",
    "scaler_cox = StandardScaler()\n",
    "X_train_128_scaled = scaler_cox.fit_transform(X_train_128_full)\n",
    "X_test_128_scaled = scaler_cox.transform(X_test_128_fixed_scaled.values)\n",
    "y_surv_full = Surv.from_arrays(event=y_event, time=y_time)\n",
    "coxph_final = CoxnetSurvivalAnalysis(l1_ratio=coxph_l1_ratio, alphas=[coxph_alpha])\n",
    "coxph_final.fit(X_train_128_scaled, y_surv_full)\n",
    "test_pred_coxph = coxph_final.predict(X_test_128_scaled)\n",
    "print(\"done\")\n",
    "\n",
    "# Model 3: DeepSurv 83 fixed\n",
    "print(\"  Training DeepSurv...\", end=\" \")\n",
    "X_train_ds_full = X_83_fixed_scaled.values\n",
    "X_test_ds = X_test_83_fixed_scaled.values\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "ds_final = DeepSurvNet(X_train_ds_full.shape[1], deepsurv_params['hidden_layers'],\n",
    "                       deepsurv_params['dropout'], deepsurv_params['activation']).to(device)\n",
    "optimizer_ds = torch.optim.Adam(ds_final.parameters(), lr=deepsurv_params['lr'], weight_decay=1e-4)\n",
    "X_tensor_full = torch.FloatTensor(X_train_ds_full).to(device)\n",
    "time_tensor_full = torch.FloatTensor(y_time).to(device)\n",
    "event_tensor_full = torch.FloatTensor(y_event.astype(float)).to(device)\n",
    "batch_size_ds = deepsurv_params['batch_size']\n",
    "n_train_full = len(X_train_ds_full)\n",
    "for epoch in range(deepsurv_params['epochs']):\n",
    "    ds_final.train()\n",
    "    indices = np.random.permutation(n_train_full)\n",
    "    for start in range(0, n_train_full, batch_size_ds):\n",
    "        end = min(start + batch_size_ds, n_train_full)\n",
    "        batch_idx = indices[start:end]\n",
    "        if len(batch_idx) < 10:\n",
    "            continue\n",
    "        optimizer_ds.zero_grad()\n",
    "        risk_pred = ds_final(X_tensor_full[batch_idx]).squeeze()\n",
    "        loss = cox_ph_loss(risk_pred, time_tensor_full[batch_idx], event_tensor_full[batch_idx])\n",
    "        loss.backward()\n",
    "        optimizer_ds.step()\n",
    "ds_final.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred_deepsurv = ds_final(torch.FloatTensor(X_test_ds).to(device)).squeeze().cpu().numpy()\n",
    "print(\"done\")\n",
    "\n",
    "# Model 4: CatBoost CLF + LightGBM REG\n",
    "print(\"  Training CatBoost CLF + LGB REG...\", end=\" \")\n",
    "X_train_tm_full = X_128_fixed_scaled.values\n",
    "X_test_tm = X_test_128_fixed_scaled.values\n",
    "clf_weights_full = compute_sample_weights(y_time, y_event.astype(int))\n",
    "cat_clf_final = CatBoostClassifier(\n",
    "    depth=int(two_model_params.get('cat_depth', 5)),\n",
    "    iterations=int(two_model_params.get('cat_iterations', 262)),\n",
    "    learning_rate=two_model_params.get('cat_learning_rate', 0.019227204273246305),\n",
    "    l2_leaf_reg=two_model_params.get('cat_l2_leaf_reg', 0.12876998314647772),\n",
    "    random_seed=seed, verbose=False, allow_writing_files=False,\n",
    ")\n",
    "cat_clf_final.fit(X_train_tm_full, y_event.astype(int), sample_weight=clf_weights_full)\n",
    "test_clf_pred = cat_clf_final.predict_proba(X_test_tm)[:, 1]\n",
    "event_mask_full = y_event == 1\n",
    "lgb_reg_final = lgb.LGBMRegressor(\n",
    "    max_depth=int(two_model_params.get('lgb_max_depth', 9)),\n",
    "    n_estimators=int(two_model_params.get('lgb_n_estimators', 149)),\n",
    "    learning_rate=two_model_params.get('lgb_learning_rate', 0.08025255147825751),\n",
    "    num_leaves=int(two_model_params.get('lgb_num_leaves', 36)),\n",
    "    min_child_samples=int(two_model_params.get('lgb_min_child_samples', 10)),\n",
    "    subsample=two_model_params.get('lgb_subsample', 0.9638486108112962),\n",
    "    colsample_bytree=two_model_params.get('lgb_colsample_bytree', 0.8126162431458441),\n",
    "    reg_alpha=two_model_params.get('lgb_reg_alpha', 4.6796458896222854e-06),\n",
    "    reg_lambda=two_model_params.get('lgb_reg_lambda', 4.190436671160808e-07),\n",
    "    random_state=seed, verbosity=-1,\n",
    ")\n",
    "lgb_reg_final.fit(X_train_tm_full[event_mask_full], y_time[event_mask_full])\n",
    "test_reg_pred = lgb_reg_final.predict(X_test_tm)\n",
    "test_pred_twomodel = merge_predictions(test_clf_pred, test_reg_pred, y_time.min(), y_time.max())\n",
    "print(\"done\")\n",
    "\n",
    "# Z-score normalize test predictions\n",
    "print(\"\\nNormalizing test predictions...\")\n",
    "test_pred_xgb_aft_norm = (test_pred_xgb_aft - test_pred_xgb_aft.mean()) / (test_pred_xgb_aft.std() + 1e-8)\n",
    "test_pred_coxph_norm = (test_pred_coxph - test_pred_coxph.mean()) / (test_pred_coxph.std() + 1e-8)\n",
    "test_pred_deepsurv_norm = (test_pred_deepsurv - test_pred_deepsurv.mean()) / (test_pred_deepsurv.std() + 1e-8)\n",
    "test_pred_twomodel_norm = (test_pred_twomodel - test_pred_twomodel.mean()) / (test_pred_twomodel.std() + 1e-8)\n",
    "\n",
    "# Combine using best weights\n",
    "w_xgb, w_cox, w_ds, w_tm = 0.50, 0.05, 0.15, 0.30\n",
    "ensemble_pred = (w_xgb * test_pred_xgb_aft_norm + \n",
    "                 w_cox * test_pred_coxph_norm + \n",
    "                 w_ds * test_pred_deepsurv_norm + \n",
    "                 w_tm * test_pred_twomodel_norm)\n",
    "\n",
    "print(f\"\\nEnsemble weights: XGB={w_xgb:.2f}, Cox={w_cox:.2f}, DS={w_ds:.2f}, TM={w_tm:.2f}\")\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'risk_score': ensemble_pred\n",
    "})\n",
    "\n",
    "# Save to outputs/submissions/\n",
    "import os\n",
    "os.makedirs('outputs/submissions', exist_ok=True)\n",
    "output_path = 'outputs/submissions/submission_ensemble_4model.csv'\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file created: {output_path}\")\n",
    "print(f\"  Shape: {submission.shape}\")\n",
    "print(f\"  Risk range: [{ensemble_pred.min():.4f}, {ensemble_pred.max():.4f}]\")\n",
    "print(f\"\\nFirst few predictions:\")\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Final Ensemble Configuration\n",
    "\n",
    "Selected weights (most even distribution at 0.6998/0.6999 level):\n",
    "\n",
    "| Component | Weight | Individual Score |\n",
    "|-----------|--------|------------------|\n",
    "| XGB AFT 83 unfixed | **50%** | 0.6964 |\n",
    "| CoxPH elastic net 128 fixed | 5% | 0.6907 |\n",
    "| DeepSurv 83 fixed | 15% | 0.6902 |\n",
    "| CatBoost CLF + LGB REG 128 fixed | **30%** | 0.6912 |\n",
    "| **Ensemble** | | **0.6998** |\n",
    "\n",
    "#### - Public leaderboard score: 0.7632\n",
    "\n",
    "## Potentially useful next steps:\n",
    "- Bagging using different seeds\n",
    "- More extensive hyperparameter tuning\n",
    "- PCA for feature engineering (preliminary analysis not useful but more extensive analysis may be)\n",
    "- Autoencoder for compressing categorical/binary features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
