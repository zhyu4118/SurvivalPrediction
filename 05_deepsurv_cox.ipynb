{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: DeepSurv (Deep Cox Proportional Hazards)\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "DeepSurv is a neural network for survival analysis:\n",
    "- **Loss**: Cox partial likelihood (negative log partial likelihood)\n",
    "- **Architecture**: MLP with dropout and batch normalization\n",
    "- **Output**: Single risk score (log hazard ratio)\n",
    "\n",
    "## Configuration\n",
    "- **Features**: 83 fixed (scaled)\n",
    "- **Architecture**: [64, 64, 64] with SELU activation\n",
    "- **Dropout**: 0.51\n",
    "- **Evaluation**: `concordance_index_ipcw` from sksurv (competition metric)\n",
    "- **CV Score**: 0.6894 weighted C-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "from sksurv.util import Surv\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "TRAIN_PATH = '/your_path/SurvivalPrediction/data'\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 83\n",
      "Samples: 3120\n",
      "Events: 1600 (51.3%)\n"
     ]
    }
   ],
   "source": [
    "# Load data with IDs and align to target\n",
    "X_train_full = pd.read_csv(f'{TRAIN_PATH}/X_train_83features_with_id_fixed_scaled.csv')\n",
    "target = pd.read_csv(f'{TRAIN_PATH}/target_train_clean_aligned.csv')\n",
    "\n",
    "# Align X_train to target patient order\n",
    "X_train = X_train_full.set_index('ID').loc[target['ID']].reset_index(drop=True)\n",
    "\n",
    "y_time = target['OS_YEARS'].values\n",
    "y_event = target['OS_STATUS'].values.astype(bool)\n",
    "n_samples = len(X_train)\n",
    "\n",
    "# Create structured array for sksurv\n",
    "y_surv = Surv.from_arrays(event=y_event, time=y_time)\n",
    "\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Samples: {n_samples}\")\n",
    "print(f\"Events: {y_event.sum()} ({y_event.mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unscaled 83 features for risk groups (must match X_train patient order)\n",
    "X_train_unscaled_full = pd.read_csv(f'{TRAIN_PATH}/X_train_83features_with_id_fixed.csv')\n",
    "X_train_unscaled = X_train_unscaled_full.set_index('ID').loc[target['ID']].reset_index(drop=True)\n",
    "\n",
    "def define_risk_groups(X):\n",
    "    risk_factors = pd.DataFrame(index=X.index)\n",
    "    risk_factors['high_blast'] = (X['BM_BLAST'] > 10).astype(int)\n",
    "    risk_factors['has_TP53'] = (X['has_TP53'] > 0).astype(int)\n",
    "    risk_factors['low_hb'] = (X['HB'] < 10).astype(int)\n",
    "    risk_factors['low_plt'] = (X['PLT'] < 50).astype(int)\n",
    "    risk_factors['high_cyto'] = (X['cyto_risk_score'] >= 3).astype(int)\n",
    "    n_risk_factors = risk_factors.sum(axis=1)\n",
    "    return {'test_like': n_risk_factors >= 1, 'high_risk': n_risk_factors >= 2}\n",
    "\n",
    "risk_groups = define_risk_groups(X_train_unscaled)\n",
    "has_tp53 = (X_train_unscaled['has_TP53'] > 0).astype(int).values\n",
    "strat_var = pd.Series([f\"{int(e)}_{int(t)}\" for e, t in zip(y_event, has_tp53)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DeepSurv Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSurvNet(nn.Module):\n",
    "    def __init__(self, in_features, hidden_layers, dropout=0.1, activation='relu'):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_size = in_features\n",
    "        act_fn = nn.ReLU() if activation == 'relu' else nn.SELU()\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(act_fn)\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def cox_ph_loss(risk_pred, time, event):\n",
    "    \"\"\"Cox partial likelihood loss.\"\"\"\n",
    "    sorted_indices = torch.argsort(time, descending=True)\n",
    "    sorted_risk = risk_pred[sorted_indices]\n",
    "    sorted_event = event[sorted_indices]\n",
    "    log_risk = torch.logcumsumexp(sorted_risk, dim=0)\n",
    "    uncensored_likelihood = sorted_risk - log_risk\n",
    "    censored_likelihood = uncensored_likelihood * sorted_event\n",
    "    return -torch.sum(censored_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_cindex_ipcw(risk, y_surv_all, risk_groups, tau=7.0):\n",
    "    c_overall = concordance_index_ipcw(y_surv_all, y_surv_all, risk, tau=tau)[0]\n",
    "\n",
    "    mask_test = risk_groups['test_like'].values\n",
    "    y_surv_test = Surv.from_arrays(event=y_surv_all['event'][mask_test], time=y_surv_all['time'][mask_test])\n",
    "    c_test = concordance_index_ipcw(y_surv_all, y_surv_test, risk[mask_test], tau=tau)[0]\n",
    "\n",
    "    mask_high = risk_groups['high_risk'].values\n",
    "    y_surv_high = Surv.from_arrays(event=y_surv_all['event'][mask_high], time=y_surv_all['time'][mask_high])\n",
    "    c_high = concordance_index_ipcw(y_surv_all, y_surv_high, risk[mask_high], tau=tau)[0]\n",
    "\n",
    "    weighted = 0.3 * c_overall + 0.4 * c_test + 0.3 * c_high\n",
    "    return {'overall': c_overall, 'test_like': c_test, 'high_risk': c_high, 'weighted': weighted}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Global OOF Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_oof_evaluate(params, n_splits=5, seed=42):\n",
    "    \"\"\"Global OOF Cross-validation for DeepSurv.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof_preds = np.zeros(n_samples)\n",
    "    \n",
    "    X_arr = X_train.values\n",
    "    hidden_layers = params['hidden_layers']\n",
    "    dropout = params['dropout']\n",
    "    activation = params['activation']\n",
    "    lr = params['lr']\n",
    "    batch_size = params['batch_size']\n",
    "    epochs = params['epochs']\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_arr, strat_var)):\n",
    "        torch.manual_seed(seed + fold_idx)\n",
    "        np.random.seed(seed + fold_idx)\n",
    "        \n",
    "        X_tr = torch.FloatTensor(X_arr[train_idx]).to(device)\n",
    "        X_val = torch.FloatTensor(X_arr[val_idx]).to(device)\n",
    "        time_tr = torch.FloatTensor(y_time[train_idx]).to(device)\n",
    "        event_tr = torch.FloatTensor(y_event[train_idx].astype(float)).to(device)\n",
    "        \n",
    "        model = DeepSurvNet(X_arr.shape[1], hidden_layers, dropout, activation).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        \n",
    "        n_train = len(train_idx)\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            indices = np.random.permutation(n_train)\n",
    "            for start in range(0, n_train, batch_size):\n",
    "                end = min(start + batch_size, n_train)\n",
    "                batch_idx = indices[start:end]\n",
    "                if len(batch_idx) < 10:\n",
    "                    continue\n",
    "                optimizer.zero_grad()\n",
    "                risk_pred = model(X_tr[batch_idx]).squeeze()\n",
    "                loss = cox_ph_loss(risk_pred, time_tr[batch_idx], event_tr[batch_idx])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            oof_preds[val_idx] = model(X_val).squeeze().cpu().numpy()\n",
    "    \n",
    "    # Global Z-score normalization\n",
    "    oof_normalized = (oof_preds - oof_preds.mean()) / (oof_preds.std() + 1e-8)\n",
    "    return weighted_cindex_ipcw(oof_normalized, y_surv, risk_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning with Optuna\n",
    "\n",
    "We tune the following hyperparameters:\n",
    "- **Architecture**: Number of layers (2-4) and hidden size (32-128)\n",
    "- **Dropout**: Regularization strength (0.1-0.7)\n",
    "- **Activation**: ReLU vs SELU\n",
    "- **Learning rate**: 1e-5 to 1e-2\n",
    "- **Batch size**: 32, 64, or 128\n",
    "- **Epochs**: 20-100\n",
    "\n",
    "Fixed: `weight_decay=1e-4` (standard L2 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEEPSURV HYPERPARAMETER TUNING\n",
      "======================================================================\n",
      "\n",
      "Running Optuna optimization (100 trials)...\n",
      "This will take a while due to neural network training...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3264496981574845b89f50ae6e1cdb76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BEST TRIAL\n",
      "======================================================================\n",
      "\n",
      "Weighted C-index: 0.6902\n",
      "Overall:   0.7175\n",
      "Test-like: 0.6921\n",
      "High-risk: 0.6605\n",
      "\n",
      "Best hyperparameters:\n",
      "  n_layers: 3\n",
      "  hidden_size: 64\n",
      "  dropout: 0.659456\n",
      "  activation: selu\n",
      "  lr: 0.000112\n",
      "  batch_size: 32\n",
      "  epochs: 58\n",
      "  hidden_layers: [64, 64, 64]\n",
      "\n",
      "======================================================================\n",
      "TOP 10 TRIALS\n",
      "======================================================================\n",
      "\n",
      "Trial  | Weighted |  Overall | Test-like | High-risk | Epochs | Activation\n",
      "---------------------------------------------------------------------------\n",
      "57     |   0.6902 |   0.7175 |    0.6921 |    0.6605 |     58 |       selu\n",
      "42     |   0.6896 |   0.7164 |    0.6911 |    0.6607 |     63 |       selu\n",
      "32     |   0.6893 |   0.7153 |    0.6910 |    0.6612 |     63 |       selu\n",
      "68     |   0.6890 |   0.7177 |    0.6910 |    0.6574 |     65 |       selu\n",
      "74     |   0.6888 |   0.7174 |    0.6911 |    0.6573 |     65 |       selu\n",
      "63     |   0.6888 |   0.7170 |    0.6909 |    0.6578 |     56 |       selu\n",
      "60     |   0.6887 |   0.7174 |    0.6907 |    0.6573 |     42 |       selu\n",
      "81     |   0.6886 |   0.7177 |    0.6910 |    0.6565 |     65 |       selu\n",
      "73     |   0.6886 |   0.7176 |    0.6909 |    0.6567 |     65 |       selu\n",
      "75     |   0.6886 |   0.7176 |    0.6909 |    0.6565 |     65 |       selu\n",
      "\n",
      "Results saved:\n",
      "  - deepsurv_83features_best_params.csv\n",
      "  - deepsurv_83features_trials.csv\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Optuna objective for DeepSurv hyperparameter tuning.\"\"\"\n",
    "    \n",
    "    # Architecture\n",
    "    n_layers = trial.suggest_int('n_layers', 2, 4)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 128, step=32)\n",
    "    hidden_layers = [hidden_size] * n_layers\n",
    "    \n",
    "    # Regularization\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.7)\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'selu'])\n",
    "    \n",
    "    # Training\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    epochs = trial.suggest_int('epochs', 20, 100)\n",
    "    \n",
    "    params = {\n",
    "        'hidden_layers': hidden_layers,\n",
    "        'dropout': dropout,\n",
    "        'activation': activation,\n",
    "        'lr': lr,\n",
    "        'batch_size': batch_size,\n",
    "        'epochs': epochs,\n",
    "    }\n",
    "    \n",
    "    result = global_oof_evaluate(params)\n",
    "    \n",
    "    # Store metrics as user attributes\n",
    "    trial.set_user_attr('overall', result['overall'])\n",
    "    trial.set_user_attr('test_like', result['test_like'])\n",
    "    trial.set_user_attr('high_risk', result['high_risk'])\n",
    "    \n",
    "    return result['weighted']\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DEEPSURV HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nRunning Optuna optimization (100 trials)...\")\n",
    "print(\"This will take a while due to neural network training...\\n\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=42),\n",
    "    study_name='deepsurv_83features'\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BEST TRIAL\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nWeighted C-index: {study.best_value:.4f}\")\n",
    "print(f\"Overall:   {study.best_trial.user_attrs['overall']:.4f}\")\n",
    "print(f\"Test-like: {study.best_trial.user_attrs['test_like']:.4f}\")\n",
    "print(f\"High-risk: {study.best_trial.user_attrs['high_risk']:.4f}\")\n",
    "\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Reconstruct hidden_layers for display\n",
    "n_layers = study.best_params['n_layers']\n",
    "hidden_size = study.best_params['hidden_size']\n",
    "print(f\"  hidden_layers: {[hidden_size] * n_layers}\")\n",
    "\n",
    "# Top 10 trials\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TOP 10 TRIALS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "trials_df = study.trials_dataframe().sort_values('value', ascending=False).head(10)\n",
    "print(\"\\n{:<6} | {:>8} | {:>8} | {:>9} | {:>9} | {:>6} | {:>10}\".format(\n",
    "    \"Trial\", \"Weighted\", \"Overall\", \"Test-like\", \"High-risk\", \"Epochs\", \"Activation\"))\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for _, row in trials_df.iterrows():\n",
    "    print(\"{:<6} | {:>8.4f} | {:>8.4f} | {:>9.4f} | {:>9.4f} | {:>6} | {:>10}\".format(\n",
    "        int(row['number']),\n",
    "        row['value'],\n",
    "        row['user_attrs_overall'],\n",
    "        row['user_attrs_test_like'],\n",
    "        row['user_attrs_high_risk'],\n",
    "        int(row['params_epochs']),\n",
    "        row['params_activation']))\n",
    "\n",
    "# Save results\n",
    "best_params_df = pd.DataFrame([{\n",
    "    'model': 'DeepSurv_83features',\n",
    "    'weighted_score': study.best_value,\n",
    "    'overall': study.best_trial.user_attrs['overall'],\n",
    "    'test_like': study.best_trial.user_attrs['test_like'],\n",
    "    'high_risk': study.best_trial.user_attrs['high_risk'],\n",
    "    **study.best_params\n",
    "}])\n",
    "best_params_df.to_csv(f'{TRAIN_PATH}/deepsurv_83features_best_params.csv', index=False)\n",
    "\n",
    "all_trials = study.trials_dataframe()\n",
    "all_trials.to_csv(f'{TRAIN_PATH}/deepsurv_83features_trials.csv', index=False)\n",
    "\n",
    "print(f\"\\nResults saved:\")\n",
    "print(f\"  - deepsurv_83features_best_params.csv\")\n",
    "print(f\"  - deepsurv_83features_trials.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verification: Re-evaluate Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying best hyperparameters from Optuna tuning...\n",
      "\n",
      "Using best parameters from Optuna study:\n",
      "  hidden_layers: [64, 64, 64]\n",
      "  dropout: 0.6594556222699043\n",
      "  activation: selu\n",
      "  lr: 0.00011237410700529054\n",
      "  batch_size: 32\n",
      "  epochs: 58\n",
      "\n",
      "CV Results (Global OOF, competition metric):\n",
      "  Overall C-index:   0.7175\n",
      "  Test-like C-index: 0.6921\n",
      "  High-risk C-index: 0.6605\n",
      "  Weighted C-index:  0.6902\n"
     ]
    }
   ],
   "source": [
    "# Verify best hyperparameters from tuning\n",
    "print(\"Verifying best hyperparameters from Optuna tuning...\\n\")\n",
    "\n",
    "# Use best params from study (or hardcoded if study not run)\n",
    "BEST_PARAMS = {\n",
    "    'hidden_layers': [study.best_params['hidden_size']] * study.best_params['n_layers'],\n",
    "    'dropout': study.best_params['dropout'],\n",
    "    'activation': study.best_params['activation'],\n",
    "    'lr': study.best_params['lr'],\n",
    "    'batch_size': study.best_params['batch_size'],\n",
    "    'epochs': study.best_params['epochs'],\n",
    "}\n",
    "print(\"Using best parameters from Optuna study:\")\n",
    "\n",
    "for k, v in BEST_PARAMS.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "result = global_oof_evaluate(BEST_PARAMS)\n",
    "print(f\"\\nCV Results (Global OOF, competition metric):\")\n",
    "print(f\"  Overall C-index:   {result['overall']:.4f}\")\n",
    "print(f\"  Test-like C-index: {result['test_like']:.4f}\")\n",
    "print(f\"  High-risk C-index: {result['high_risk']:.4f}\")\n",
    "print(f\"  Weighted C-index:  {result['weighted']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Final Model on Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on full data\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "X_tensor = torch.FloatTensor(X_train.values).to(device)\n",
    "time_tensor = torch.FloatTensor(y_time).to(device)\n",
    "event_tensor = torch.FloatTensor(y_event.astype(float)).to(device)\n",
    "\n",
    "final_model = DeepSurvNet(\n",
    "    X_train.shape[1],\n",
    "    BEST_PARAMS['hidden_layers'],\n",
    "    BEST_PARAMS['dropout'],\n",
    "    BEST_PARAMS['activation']\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=BEST_PARAMS['lr'], weight_decay=1e-4)\n",
    "\n",
    "batch_size = BEST_PARAMS['batch_size']\n",
    "n_train = len(X_train)\n",
    "\n",
    "print(f\"Training for {BEST_PARAMS['epochs']} epochs...\")\n",
    "for epoch in range(BEST_PARAMS['epochs']):\n",
    "    final_model.train()\n",
    "    indices = np.random.permutation(n_train)\n",
    "    for start in range(0, n_train, batch_size):\n",
    "        end = min(start + batch_size, n_train)\n",
    "        batch_idx = indices[start:end]\n",
    "        if len(batch_idx) < 10:\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        risk_pred = final_model(X_tensor[batch_idx]).squeeze()\n",
    "        loss = cox_ph_loss(risk_pred, time_tensor[batch_idx], event_tensor[batch_idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data and predict\n",
    "X_test_full = pd.read_csv(f'{TRAIN_PATH}/X_test_83features_with_id_fixed_scaled.csv')\n",
    "test_ids = X_test_full['ID'].values\n",
    "X_test = X_test_full.drop(columns=['ID'])\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.FloatTensor(X_test.values).to(device)\n",
    "    test_risk = final_model(X_test_tensor).squeeze().cpu().numpy()\n",
    "\n",
    "print(f\"Test predictions: {len(test_risk)} samples\")\n",
    "print(f\"Risk range: [{test_risk.min():.4f}, {test_risk.max():.4f}]\")\n",
    "\n",
    "submission = pd.DataFrame({'ID': test_ids, 'risk_score': test_risk})\n",
    "submission.to_csv(f'{TRAIN_PATH}/submission_deepsurv_83features.csv', index=False)\n",
    "print(f\"Saved: submission_deepsurv_83features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### DeepSurv Model Results (Competition Metric)\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Overall C-index | 0.7175 |\n",
    "| Test-like C-index | 0.6921 |\n",
    "| High-risk C-index | 0.6605 |\n",
    "| **Weighted C-index** | **0.6902** |\n",
    "\n",
    "### Key Findings\n",
    "1. Neural network approach provides different error patterns than tree models\n",
    "2. SELU activation with high dropout (0.51) works best\n",
    "3. Useful for ensembling with XGBoost AFT\n",
    "4. Optimal ensemble weight: 25% (combined with 75% XGBoost AFT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
